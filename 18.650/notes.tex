\documentclass{article}

\title{Stat 110}
\author{Notes by Edward Chen}
\date{October 2022}

\begin{document}
    \maketitle
    \section*{Preface}

These are my notes to Harvard's Stat 110 class,
taught by Joseph Blitzstein and Jessica Hwang. The class covers all the basics of probability--counting principles,
probabilistic events, random variables, distributions, conditional probability, expectation, and Bayesian inference.\\

    Compared to other notes, this should not be used as a whole subsititue for the class, rather as an indication of the more complex topics covered.
    Put harsher, I will likely spend less time note taking on things I already know and more on things I'm confused out. For clarity's sake,
    I am pretty comfortable with probability from my competition math background.

    Lecture videos are freely available at 


    \section{Chapter 1}

    Probability gives us a logical framework to view and anlyze uncertaintiy. It is the foundation and language for statistic ass well as the basis
        for topics such as Statistics, Physics, Biology, and Computer science.\\\\

    A \textbf{sample space} $S$ is the set of all theoretically possible outcomes.\\\\

    Let $D$ be the set of all coin flips with at least two consecutive heads. The sameple space,
     expressed as a set would be:
        $$D = \cup_{j=1}^9(A_j\cap A_{j+1})$$

    Some more set notation:\\
        sameple space: $S$\\
        s is a possible outcome: $s\in S$\\
        A is an event: $A 17 S$\\
        A implies B: \\
        A and B are mutually exclusive: $A\cap B = \emptyset$\\\\

    Example 1.4.10: Birthday Problem

    $\textbf{Theorem 1.4.15}$ (Binomial coefficient formula). For $k\le n$, we have:
    $$ {n\choose k} = \frac{n(n-1)...(n-k+1)}{k!} = \frac{n!}{(n-k)!k!}$$

    Some proof methods covered in the chapter include complementary counting, stars and bars, and story proofs(1.5).\\

    There are a lot of formulas with binomial coefficients:\\
        $${n\choose k} = {n-k\choose k}$$\\
        $$(x+y)^n = \sum_{k=0}^n {n\choose k}x^ky^{n-k}$$\\
        $$n{n-1 \choose k-1} = k{n\choose k}$$\\
        $$\textrm{Vandermonde's}: {m+n\choose k}= \sum_{j=0}^k{m\choose j}{n\choose k-j}$$

    $\textbf{Example 1.5.4}$(Partnerships). Let's prove\\

    $$\frac{(2n)!}{2^n\cdot n!}=(2n-1)(2n-3)...3\cdot1$$

    $\textit{Story proof:}$: We will show that both sides count the number of ways to break $2n$
        people into n partnerships. Take $2n$ people, and give them ID numbers from 1 to $2n$. We can form
        partnerships by lining up the people in some order and then saying the first two
        are a pair, the next two are a pair, etc. This overcounts by a factor of $n!\cdot 2^n$ since 
        the order of pairs doesn't matter, nor does the order within each pair. Alternaitvely, count the 
        number of possibilities by noting that there are $2n-1$ choices for the parnter of person 1, then
        $2n-3$ choices for person 2, and so on.

    $\textbf{Definition 1.6.1}$ (General definition of probability). A $\textit{probability space}$ consists of a sample 
        space $S$ and a $\textit{probability function } P$ and returns a real number between 0 and 1, $P(A)$, where 
        $A$ is the event it takes in.

    Unlike the naive definition, here we can have events with different probabilities.
    
    The $\textit{frequentist}$ view of probability is that it represents a long-run frequency over
        a large number of repetitions of an experiment. The $\textit{Bayesian view}$ is that it represents a
        degree of belief about the event in question, so we can assign probabilities to hypotheses.

    Inclusion-exclusion example: With a triple venn diagram, we can write:
    $$P(A\cup B\cup C)=P(A)+P(B)+P(C) - P(A\cap B)-P(A\cap C)-P(B\cap C)+P(A\cup B \cup C)$$
    
    Generally, we can write:



    $\textbf{Example 1.6.4}$ (de Montmort's matching problem). Consider a shuffled deck of $n$ cards, from 1 to n.
    You flip each one over one by one. What is the probability the $i$th index card
    you turn over has value $i$?

    With PIE, we get $$P(\cup_{i=1}^n A_i)=\frac{n}{n}-\frac{{n\choose 2}}{n(n-1)}
    +\frac{{n\choose 3}}{n(n-1)(n-2)}- ...+(-1)^{n+1}\cdot\frac{1}{n!}$$\\
    $$= 1-\frac{1}{2!}+\frac{1}{3!}-...+(-1)^{n+1}\cdot\frac{1}{n!}$$

    With large $n$, this approaches the Taylor series for $\frac{1}{e}$:

    $$e^{-1}=1-\frac{1}{1!}+\frac{1}{2!}-\frac{1}{3!}+...$$\\\\

    $\textbf{R}$: Please read section 1.8 for an introduction to R. R allows us to simulate and deal with large sets of data.

    \pagebreak
    
    \section{Chapter 2: Conditional Probability}

    $\textit{Conditional Probability}$ explores how we should update our probability when we receive new information.

    $\textbf{Definition 2.2.1}$. If $A$ and $B$ are events with $P(B)>0$, then conditional probability can be expressed as:
    $$P(A|B) = \frac{P(A\cap B)}{P(B)}$$

    $\textbf{Example 2.2.5}$(Two Children): Martin Gardner posed the following puzzle:\\
    $\textit{Mr. Jones has two children. The older hild is a girl. What is the probability that both children are girls?}$\\
    $\textit{Mr. Smith has two children. At least one of them is aboy. What is the probability that both children are boys?}$\\

    The intuition is that they should both be $\frac{1}{2}$, but the respesctive probabilities are actually $\frac{1}{2}$ and $\frac{1}{3}$.\\\\

    $\textbf{Theorem 2.23}$ (Bayes' rule).
    $$P(A|B)=\frac{P(A\cap B)}{B}=\frac{P(B|A)P(A)}{P(B)}$$

    $\textbf{Definition 2.3.4}$ (Odds). The $\textit{odds}$ of an event A are

    $$\textrm{odds(A)}=P(A)/P(A^c)$$. Ex.: if $P(A)=2/3$, we say the odds in favor of A are 2 to 1.

    $\textbf{Theorem 2.3.5}$ (Odds form of Bayes' rule)

    $\textbf{Example 2.4.5}$ (Unanimous agreement). The artaicle "Why too much evidence can be a bad thing" by Lisa Zyga says:\\

    $\textit{Under ancient Jewish law, if a suspect on trial was unanimously found guilty by all judges, then the suspect was acquitted. This reasoning
    sounds counterintuitive, but the legislators of teh time had noticed that unanimous agreement often indicates the presence of systemic error in 
    the judicial process}.$

    This is because a systemic error occurs independet of the conviction, so there may be an unseen bias at play here.

    $\textbf{Independence}$

    Events $A$ and $B$ are $\textit{independent}$ if $P(A\cap B) = P(A)P(B)$.

    For infinitely many events, we say that they are independent if every finite subset of the events is independent.

    $\textbf{Definition 2.5.7}$ (Conditional independence). Events $A$ and $B$ are said to be $\textit{conditionally independent}$ given $E$ if $P(A\cap B|E) = P(A|E)P(B|E)$.

    \begin{quote}
    It is easy to make terrible blunders stemming from confusing independence and conditional independence. Two events can be conditionally independence given $E$, but not independent given $E^c$. Two events
    can be conditionally independent given $E$, but not independent. Two events can be independent, but not conditionally independent given $E$.
    Great care is needed
    \end{quote}

    todo: 65-67

    An important property of Bayes' rule is that is is $coherent$: for the same updates, no matter the update size, the final state should be the same.

    todo: 68-75\\\\

    $\textbf{Example 2.7.1}$ (Monty Hall):

    Here the strategy is to use conditioning as a problem-solving tool. If we knew where the car actually is, it makes the probabilities a lot easier to deal with:

    $P(\textrm{get car})=P(\textrm{get car}|C_1)\cdot\frac{1}{3}+P(\textrm{get car}|C_2)\cdot\frac{1}{3}+P(\textrm{get car}|C_3)\cdot\frac{1}{3} = \frac{2}{3}$

    That is why it is always better to switch because now you have a 2/3 success strategy.

    $\textbf{Example 2.7.3}$ (Gambler's ruin):

    The gambler's ruin problem is a problem where we have two players, one with $n$ dollars and the other with $n$ dollars. They play a game where they bet 1 dollar each. There is probability of $p$  that the first player wins and gets 2 dollars, and otherwise the second player gets the 2 dollars. The game ends when one of the players has 0 dollars. What is the probability that the first player wins?\\\\

    This can be solved by conditioning on the first step and is just a general version of the random walk problem. We have $\textbf{absorbing state}$ at $0$ and $2n$ and we have the relationship:
        $$p_i=p\cdot p_{i+1}+(1-p)\cdot p_{i-1}$$\\

    To solve, we start by guessing $p_i=x^i$. Then we get:
        $$x^i=p\cdot x^{i+1}+(1-p)\cdot x^{i-1}$$\\
        $$px^2-x+(1-p)=0$$\\
        $$x=\frac{1\pm\sqrt{1-4p(1-p)}}{2p}$$\\

    \section{Chapter 3: Random Variables}

    A $\textbf{random variable}$ is more than "a variable that takes on random values." A better definition is that a random variable is a function that maps a sample space to the real numbers.\\

    $\textbf{Definition 3.1.2}$ (Bernoulli distribution). The $\textbf{Bernoulli distribution}$ is the probability distribution of a random variable $X$ that takes on the values $0$ and $1$ with probabilities $p$ and $1-p$, respectively. We write $X ~ Bern(n,p)$ as $X$ is distributed as a Bernoulli function with $n$ independent Bernoulli random variables that have probability $p$ of being 1.\\

    Interestingly: the binomial distribution is the same distribution of the sum of $n$ independent Bernoulli random variables. \\

    Todo: proposition 9.2

    $\textbf{Definition 3.2.2}$ (Probability mass function). The $\textbf{probability mass function}$ of a discrete random variable $X$ is a function $f_X$ that assigns to each value $x$ of $X$ the probability that $X$ takes on the value $x$.\\

    $\textbf{Definition 3.2.3}$ (Cumulative distribution function). The $\textbf{cumulative distribution function}$ of a discrete random variable $X$ is a function $F_X$ that assigns to each value $x$ of $X$ the probability that $X$ takes on a value less than or equal to $x$.\\

    $\textbf{Hypergeometric distribution}$ is the probability distribution of the number of successes in a sample of size $n$ drawn with replacement from a finite population of size $N$ containing exactly $K$ successes.\\

    $\textbf{Theorem 3.4.2}$ (Hypergeometric PMF): The probability mass function of a hypergeometric random variable $X$ is given by:
    % $$f_X(x)=\frac{({K\choose x}{ N-K\choose n-x})}{N\choose n}$$

    Hypergeometric distributions are very similar to binomial distrbutions in that they take on $n$ Bernoulli trials, but note that hypergeometric distributions are dependent as we are drawing without replacement.\\




\end{document}

